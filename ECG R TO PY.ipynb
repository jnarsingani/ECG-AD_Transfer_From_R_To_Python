{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T23:17:57.026841Z",
     "start_time": "2025-12-24T23:17:57.024311Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Setting-Up Random seed\n",
    "random_seed = 123\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c64b70522eed61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T23:19:03.846539Z",
     "start_time": "2025-12-24T23:19:02.525060Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"aami2_rr.csv\")\n",
    "print(f\"Number of rows: {df.shape[0]} and Number of Columns: {df.shape[1]}\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481922b71a34901b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T23:19:06.219080Z",
     "start_time": "2025-12-24T23:19:06.197502Z"
    }
   },
   "outputs": [],
   "source": [
    "# Unique Values\n",
    "print(df['annotations'].value_counts())\n",
    "\n",
    "# Creating binary label: 0: Normal Heart Beat, 1: Abnormal/Anything except normal\n",
    "df['label'] = df['annotations'].apply(lambda x: 0 if x == 'N' else 1)\n",
    "\n",
    "# Label Distribution\n",
    "label_count  = df['label'].value_counts().sort_index()\n",
    "print(f\"Normal (0):   {label_count[0]:,} samples ({label_count[0]/len(df)*100:.2f}%)\")\n",
    "print(f\"Abnormal (1): {label_count[1]:,} samples ({label_count[1]/len(df)*100:.2f}%)\")\n",
    "\n",
    "# Contamination rate (For isolation forest)\n",
    "contamination_rate = label_count[1] / len(df)\n",
    "print(f\"Contamination ({contamination_rate*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f388be4c8604553a",
   "metadata": {},
   "source": [
    "The contamination parameter informs Isolation Forest's threshold for classifying anomalies based on the known proportion of abnormal samples in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe7df88713a6476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify feature columns \n",
    "feature_columns = [col for col in df.columns if col.startswith('X')]\n",
    "n_features = len(feature_columns)\n",
    "print(f\"Number of features: {n_features}\")\n",
    "\n",
    "# check for missing values\n",
    "missing = df[feature_columns].isnull().sum().sum()\n",
    "print(f\"Number of missing values: {missing}\")\n",
    "\n",
    "# Range of features (Important for knowing if we need scaling)\n",
    "print(f\"Minimum: {df[feature_columns].min().min():.4f}\")\n",
    "print(f\"Maximum: {df[feature_columns].max().max():.4f}\")\n",
    "\n",
    "print(\"Variance per feature (first 10):\")\n",
    "print(df[feature_columns[:10]].var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c5141e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot 1: Sample normal heartbeat\n",
    "plt.subplot(1, 2, 1)\n",
    "normal_sample = df[df['label']==0][feature_columns].iloc[0]\n",
    "plt.plot(normal_sample, color='green', linewidth=1.5, alpha=0.7)\n",
    "plt.title('Normal Heartbeat Pattern (Sample)', fontweight='bold')\n",
    "plt.xlabel('Feature Index (X1 to X150)')\n",
    "plt.ylabel('R-R Interval Value')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Sample abnormal heartbeat  \n",
    "plt.subplot(1, 2, 2)\n",
    "abnormal_sample = df[df['label']==1][feature_columns].iloc[0]\n",
    "plt.plot(abnormal_sample, color='red', linewidth=1.5, alpha=0.7)\n",
    "plt.title('Abnormal Heartbeat Pattern (Sample)', fontweight='bold')\n",
    "plt.xlabel('Feature Index (X1 to X150)')\n",
    "plt.ylabel('R-R Interval Value')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074e6c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a summary\n",
    "print(\"=\"*70)\n",
    "print(\"SUMMARY - DATA EXPLORATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n✓ Total Samples: {len(df):,}\")\n",
    "print(f\"✓ Features (R-R intervals): {n_features}\")\n",
    "print(f\"✓ Normal heartbeats: {label_count[0]:,} ({label_count[0]/len(df)*100:.1f}%)\")\n",
    "print(f\"✓ Abnormal heartbeats: {label_count[1]:,} ({label_count[1]/len(df)*100:.1f}%)\")\n",
    "print(f\"✓ Contamination rate: {contamination_rate:.4f}\")\n",
    "print(f\"✓ Feature ranges: {df[feature_columns].min().min():.2f} to {df[feature_columns].max().max():.2f}\")\n",
    "print(f\"✓ Missing values: {missing}\")\n",
    "print(f\"\\n⚠ Key insight: Features have different variances → Need standardization!\")\n",
    "print(f\"✓ Visual patterns differ between normal and abnormal\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1990d2d7",
   "metadata": {},
   "source": [
    "# Step 2: Data Processing and Feature Scaling\n",
    "\n",
    "## Objectives:\n",
    "1. Split data into training and testing sets\n",
    "2. Stadardize features (Zero mean, unit variance)\n",
    "3. Understand why we fit scaler on training data only \n",
    "4. Prepare data for Random Forest and Isolation Forest \n",
    "5. Save process data for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c4bb3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature (X) shape: (88788, 150)\n",
      "Label (y) shape: (88788,)\n"
     ]
    }
   ],
   "source": [
    "# Seperate features (X) and labels (y)\n",
    "X = df[feature_columns].values\n",
    "y = df['label'].values\n",
    "\n",
    "print(f\"Feature (X) shape: {X.shape}\")\n",
    "print(f\"Label (y) shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "064a4565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (44394, 150)\n",
      "Test set: (44394, 150)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# Split data: 50% train, 50% test \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5, random_state = random_seed, stratify = y)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "945f4b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training data:\n",
      "  Mean: 0.0097\n",
      "  Std:  0.3468\n",
      "\n",
      "Standardized training data:\n",
      "  Mean: 0.0000\n",
      "  Std:  1.0000\n",
      "\n",
      "Standardized test data:\n",
      "  Mean: 0.0016\n",
      "  Std:  1.0205\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "# Scaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler only on the training data\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Transform training and testing data\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Original training data:\")\n",
    "print(f\"  Mean: {X_train.mean():.4f}\")\n",
    "print(f\"  Std:  {X_train.std():.4f}\")\n",
    "\n",
    "print(f\"\\nStandardized training data:\")\n",
    "print(f\"  Mean: {X_train_scaled.mean():.4f}\")\n",
    "print(f\"  Std:  {X_train_scaled.std():.4f}\")\n",
    "\n",
    "print(f\"\\nStandardized test data:\")\n",
    "print(f\"  Mean: {X_test_scaled.mean():.4f}\")\n",
    "print(f\"  Std:  {X_test_scaled.std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51e7538",
   "metadata": {},
   "source": [
    "We Fit the scaler exclusively on training data to prevent information leakage, which is why test data statistics deviate slightly from 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2ab65e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save preprocessed data for model training \n",
    "preprocessed_data = {\n",
    "    'X_train': X_train,\n",
    "    'X_test': X_test,\n",
    "    'y_train': y_train,\n",
    "    'y_train':y_train,\n",
    "    'X_train_scaled': X_train_scaled,\n",
    "    'X_test_scaled': X_test_scaled,\n",
    "    'contamination_rate': contamination_rate,\n",
    "    'feature_cols': feature_columns,\n",
    "    'random_seed': random_seed\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "with open('preprocessed_data.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessed_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b998b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
