{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68dccc34",
   "metadata": {},
   "source": [
    "# Step 4: Isolation Forest Variants\n",
    "\n",
    "**Learning Objectives:**\n",
    "1. Understand how Isolation Forest works (unsupervised anomaly detection)\n",
    "2. Implement Standard Isolation Forest\n",
    "3. Compare with Random Forest results\n",
    "4. Understand contamination parameter\n",
    "5. Analyze sensitivity vs specificity tradeoffs\n",
    "\n",
    "---\n",
    "\n",
    "## What is Isolation Forest?\n",
    "\n",
    "**Key Concept:** Anomalies are \"easier to isolate\" than normal points.\n",
    "\n",
    "**How it works:**\n",
    "- Build random decision trees\n",
    "- Anomalies get isolated with fewer splits\n",
    "- Gives each point an \"anomaly score\"\n",
    "- Higher score = more likely to be anomaly\n",
    "\n",
    "**Key Parameter:**\n",
    "- `contamination`: Expected proportion of anomalies (we use 0.101 = 10.1%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "277c7432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data and the libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import pickle\n",
    "from sklearn.ensemble import IsolationForest \n",
    "from sklearn.metrics import confusion_matrix, accuracy_score \n",
    "import time \n",
    "\n",
    "# Seed\n",
    "random_seed = 123\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# Load preprocessed data\n",
    "with open('preprocessed_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Extract the needed data \n",
    "X_train_scaled = data['X_train_scaled']  # Isolation Forest needs scaled data!\n",
    "X_test_scaled = data['X_test_scaled']\n",
    "y_train = data['y_train']\n",
    "y_test = data['y_test']\n",
    "contamination_rate = data['contamination_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b1db7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done   2 out of  16 | elapsed:    0.0s remaining:    0.9s\n",
      "[Parallel(n_jobs=16)]: Done  16 out of  16 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model completed in 0.39 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "# Train Standard Isolation Forest\n",
    "\n",
    "iso_forest = IsolationForest(\n",
    "    n_estimators = 100,            # Number of trees\n",
    "    max_samples = 1000,            # Sample size per tree\n",
    "    contamination = contamination_rate, # Expected proportion of anomalies\n",
    "    random_state = random_seed,\n",
    "    n_jobs = -1,\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "# Train the model \n",
    "start_time = time.time()\n",
    "\n",
    "iso_forest.fit(X_train_scaled)\n",
    "\n",
    "training_time = time.time()-start_time\n",
    "print(f\"Training model completed in {training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d9b8737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Predictions:\n",
      "  Normal (0):   39,691\n",
      "  Abnormal (1): 4,703\n",
      "\n",
      "Expected abnormal (based on contamination): 4,733\n",
      "Actual predicted abnormal: 4,703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "# Making prediction \n",
    "\n",
    "# Predict on test set\n",
    "# Returns -1 for anomalies, 1 for normal - Sklearn convention \n",
    "y_pred_if_raw = iso_forest.predict(X_test_scaled)\n",
    "\n",
    "# Convert to our format: 0 = Normal, 1 = Anomaly \n",
    "# sklearn: 1=normal, -1=anomaly\n",
    "# We need: 0=normal, 1=anomaly\n",
    "y_pred_if = np.where(y_pred_if_raw == -1, 1, 0)\n",
    "\n",
    "# Counting prediction \n",
    "unique, counts = np.unique(y_pred_if, return_counts = True)\n",
    "pred_dict = dict(zip(unique, counts))\n",
    "\n",
    "print(f\"\\n Predictions:\")\n",
    "print(f\"  Normal (0):   {pred_dict.get(0, 0):,}\")\n",
    "print(f\"  Abnormal (1): {pred_dict.get(1, 0):,}\")\n",
    "\n",
    "# Expected abnormal based on contamination\n",
    "expected_abnormal = int(len(y_test) * contamination_rate)\n",
    "print(f\"\\nExpected abnormal (based on contamination): {expected_abnormal:,}\")\n",
    "print(f\"Actual predicted abnormal: {pred_dict.get(1, 0):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4ba6cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Normal     36689    2972\n",
      "       Abnormal    3002    1731\n",
      "\n",
      "======================================================================\n",
      "METRICS:\n",
      "======================================================================\n",
      "Accuracy:    86.54%\n",
      "Sensitivity: 36.57%  (catches 36.6% of abnormal)\n",
      "Specificity: 92.51%  (catches 92.5% of normal)\n",
      "\n",
      "======================================================================\n",
      "COMPARISON:\n",
      "======================================================================\n",
      "Random Forest:      Acc=98.14%, Sens=84.62%, Spec=99.75%\n",
      "Isolation Forest:   Acc=86.54%, Sens=36.57%, Spec=92.51%\n"
     ]
    }
   ],
   "source": [
    "# Calculate performance matricies \n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Calculate metrics \n",
    "accuracy = accuracy_score(y_test, y_pred_if)\n",
    "\n",
    "# Confusion matrix \n",
    "conf_matrix = confusion_matrix(y_test, y_pred_if)\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "# Calculate sensitivity and specificity \n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(f\"Actual Normal    {tn:6d}  {fp:6d}\")\n",
    "print(f\"       Abnormal  {fn:6d}  {tp:6d}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"METRICS:\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Accuracy:    {accuracy*100:.2f}%\")\n",
    "print(f\"Sensitivity: {sensitivity*100:.2f}%  (catches {sensitivity*100:.1f}% of abnormal)\")\n",
    "print(f\"Specificity: {specificity*100:.2f}%  (catches {specificity*100:.1f}% of normal)\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"COMPARISON:\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Random Forest:      Acc={98.14:.2f}%, Sens={84.62:.2f}%, Spec={99.75:.2f}%\")\n",
    "print(f\"Isolation Forest:   Acc={accuracy*100:.2f}%, Sens={sensitivity*100:.2f}%, Spec={specificity*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c35ac26",
   "metadata": {},
   "source": [
    "\n",
    "## Single-Variable Isolation Forest\n",
    "\n",
    "**Key Idea:** Instead of one model using all 150 features, train 150 separate models - one per feature!\n",
    "\n",
    "**How it works:**\n",
    "1. Train 150 individual Isolation Forests (one for each R-R interval feature)\n",
    "2. Each model gives an anomaly score\n",
    "3. Average all 150 scores\n",
    "4. Higher average score = more likely anomaly\n",
    "\n",
    "\n",
    "**Expected from R:** Sensitivity 92.52%, Specificity 31.45%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e42430c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 150 individual isolation forest models.\n",
      " Processed 30 / 150 features.\n",
      " Processed 60 / 150 features.\n",
      " Processed 90 / 150 features.\n",
      " Processed 120 / 150 features.\n",
      " Processed 150 / 150 features.\n",
      "Training completed in 15.59 seconds.\n",
      "Total trees: 3000 = 3000\n"
     ]
    }
   ],
   "source": [
    "# Train 150 models - One per feature \n",
    "\n",
    "n_features = X_train_scaled.shape[1] \n",
    "single_models = []\n",
    "feature_scores = np.zeros((X_test_scaled.shape[0], n_features))\n",
    "\n",
    "print(f\"Training {n_features} individual isolation forest models.\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(n_features):\n",
    "    if(i + 1) % 30 == 0:      # Progress update every 30 models\n",
    "        print(f\" Processed {i+1} / {n_features} features.\")\n",
    "    \n",
    "    # Training IF on single feature\n",
    "    single_if = IsolationForest(\n",
    "        n_estimators = 20,           # 20 Trees per model\n",
    "        max_samples = 500,\n",
    "        contamination = contamination_rate,\n",
    "        random_state = random_seed\n",
    "    )\n",
    "\n",
    "    # Fit on a single feature\n",
    "    X_train_single = X_train_scaled[:, i].reshape(-1,1)\n",
    "    single_if.fit(X_train_single)\n",
    "\n",
    "    # Predict on test set for this feature\n",
    "    X_test_single = X_test_scaled[:, i].reshape(-1,1)\n",
    "    predictions = single_if.predict(X_test_single)\n",
    "\n",
    "    # Convert to 0/1 and store\n",
    "    feature_scores[:, i] = np.where(predictions == -1, 1, 0)\n",
    "    single_models.append(single_if)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"Training completed in {training_time:.2f} seconds.\")\n",
    "print(f\"Total trees: {n_features * 20} = {n_features * 20}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afeb0ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average scores for test samples: 44,394\n",
      "\n",
      "Final Predictions:\n",
      "  Normal (0):   39,645\n",
      "  Abnormal (1): 4,749\n"
     ]
    }
   ],
   "source": [
    "# Average the pridictions across all 150 features \n",
    "# Eech raw = one test sample\n",
    "# Each column = one feature's vote (0 or 1)\n",
    "# Average = What % of features say 'abnormal'\"ECG R TO PY.ipynb\"\n",
    "\n",
    "average_scores = feature_scores.mean(axis=1)\n",
    "\n",
    "print(f\"Average scores for test samples: {len(average_scores):,}\")\n",
    "\n",
    "# Determine threshold based on contamination\n",
    "# If contamination = 0.101, we want top 10.1% to be labled abnormal \n",
    "\n",
    "threshold = np.quantile(average_scores, 1 - contamination_rate)\n",
    "\n",
    "# Final prediction\n",
    "y_pred_single_var = np.where(average_scores >= threshold, 1, 0)\n",
    "\n",
    "# Count predictions\n",
    "pred_normal = sum(y_pred_single_var == 0)\n",
    "pred_abnormal = sum (y_pred_single_var == 1)\n",
    "\n",
    "print(f\"\\nFinal Predictions:\")\n",
    "print(f\"  Normal (0):   {pred_normal:,}\")\n",
    "print(f\"  Abnormal (1): {pred_abnormal:,}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99fdb16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Normal     36241    3404\n",
      "       Abnormal    3420    1329\n",
      "Accuracy:    84.63%\n",
      "Sensitivity: 27.98%  (catches 28.0% of abnormal)\n",
      "Specificity: 91.41%  (catches 91.4% of normal)\n",
      "R (from paper):\n",
      "  Single-Variable IF:  Acc=85.70%, Sens=92.52%, Spec=31.45%\n",
      "\n",
      "Python (current):\n",
      "  Random Forest:       Acc=98.14%, Sens=84.62%, Spec=99.75%\n",
      "  Standard IF:         Acc=86.54%, Sens=36.57%, Spec=92.51%\n",
      "  Single-Variable IF:  Acc=84.63%, Sens=27.98%, Spec=91.41%\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics \n",
    "accuracy_sv = accuracy_score(y_test, y_pred_single_var)\n",
    "\n",
    "# confusion matrix \n",
    "cm_sv = confusion_matrix(y_test, y_pred_single_var)\n",
    "tn_sv, fn_sv, fp_sv, tp_sv = cm_sv.ravel()\n",
    "\n",
    "# Calculate sensitivity and specificity\n",
    "sensitivity_sv = tp_sv / (tp_sv + fn_sv)\n",
    "specificity_sv = tn_sv / (tn_sv + fp_sv)\n",
    "\n",
    "print(f\"Actual Normal    {tn_sv:6d}  {fp_sv:6d}\")\n",
    "print(f\"       Abnormal  {fn_sv:6d}  {tp_sv:6d}\")\n",
    "\n",
    "print(f\"Accuracy:    {accuracy_sv*100:.2f}%\")\n",
    "print(f\"Sensitivity: {sensitivity_sv*100:.2f}%  (catches {sensitivity_sv*100:.1f}% of abnormal)\")\n",
    "print(f\"Specificity: {specificity_sv*100:.2f}%  (catches {specificity_sv*100:.1f}% of normal)\")\n",
    "\n",
    "print(f\"R (from paper):\")\n",
    "print(f\"  Single-Variable IF:  Acc=85.70%, Sens=92.52%, Spec=31.45%\")\n",
    "print(f\"\\nPython (current):\")\n",
    "print(f\"  Random Forest:       Acc=98.14%, Sens=84.62%, Spec=99.75%\")\n",
    "print(f\"  Standard IF:         Acc=86.54%, Sens=36.57%, Spec=92.51%\")\n",
    "print(f\"  Single-Variable IF:  Acc={accuracy_sv*100:.2f}%, Sens={sensitivity_sv*100:.2f}%, Spec={specificity_sv*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5ad427",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## SCiForest (Sparse-Cut Isolation Forest)\n",
    "\n",
    "**Key Idea:** Use random subsets of features for each model\n",
    "\n",
    "**How it works:**\n",
    "1. Train multiple models (we'll use 50 models)\n",
    "2. Each model uses only 75 random features (out of 150)\n",
    "3. Each model has 10 trees\n",
    "4. Average all anomaly scores\n",
    "\n",
    "**Why this might work:**\n",
    "- Reduces feature dominance\n",
    "- Forces diversity across models\n",
    "- Feature sparsity can improve generalization\n",
    "\n",
    "**Expected from R:** Accuracy 84.68%, Sensitivity 90.17%, Specificity 30.98%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44e23d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SCIFOREST (SPARSE-CUT ISOLATION FOREST)\n",
      "======================================================================\n",
      "\n",
      "Training 50 models...\n",
      "Each model uses 75 random features\n",
      "Each model has 10 trees\n",
      "  Training model 10/50...\n",
      "  Training model 20/50...\n",
      "  Training model 30/50...\n",
      "  Training model 40/50...\n",
      "  Training model 50/50...\n",
      "\n",
      "✓ Training complete in 4.38 seconds\n",
      "✓ Total: 50 models × 10 trees = 500 trees\n",
      "\n",
      "======================================================================\n",
      "SCIFOREST PERFORMANCE:\n",
      "======================================================================\n",
      "Accuracy:    86.25%\n",
      "Sensitivity: 36.04%\n",
      "Specificity: 92.24%\n",
      "\n",
      "R result: Acc=84.68%, Sens=90.17%, Spec=30.98%\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"SCIFOREST (SPARSE-CUT ISOLATION FOREST)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "n_models = 50\n",
    "n_features_per_model = 75  # Use half of 150 features\n",
    "\n",
    "print(f\"\\nTraining {n_models} models...\")\n",
    "print(f\"Each model uses {n_features_per_model} random features\")\n",
    "print(f\"Each model has 10 trees\")\n",
    "\n",
    "sci_scores = np.zeros(X_test_scaled.shape[0])\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(n_models):\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"  Training model {i + 1}/{n_models}...\")\n",
    "    \n",
    "    # Randomly select 75 features\n",
    "    selected_features = np.random.choice(150, n_features_per_model, replace=False)\n",
    "    \n",
    "    # Train model on selected features\n",
    "    sci_model = IsolationForest(\n",
    "        n_estimators=10,  # 10 trees per model\n",
    "        max_samples=1000,\n",
    "        contamination=contamination_rate,\n",
    "        random_state= random_seed + i  # Different seed for each model\n",
    "    )\n",
    "    \n",
    "    # Fit on training data with selected features\n",
    "    sci_model.fit(X_train_scaled[:, selected_features])\n",
    "    \n",
    "    # Predict on test data with same features\n",
    "    predictions = sci_model.predict(X_test_scaled[:, selected_features])\n",
    "    \n",
    "    # Convert and accumulate scores\n",
    "    sci_scores += np.where(predictions == -1, 1, 0)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "# Average scores across all models\n",
    "sci_scores = sci_scores / n_models\n",
    "\n",
    "# Apply threshold\n",
    "threshold = np.quantile(sci_scores, 1 - contamination_rate)\n",
    "y_pred_sci = np.where(sci_scores >= threshold, 1, 0)\n",
    "\n",
    "print(f\"\\n✓ Training complete in {training_time:.2f} seconds\")\n",
    "print(f\"✓ Total: {n_models} models × 10 trees = {n_models * 10} trees\")\n",
    "\n",
    "# Evaluate\n",
    "accuracy_sci = accuracy_score(y_test, y_pred_sci)\n",
    "cm_sci = confusion_matrix(y_test, y_pred_sci)\n",
    "tn_sci, fp_sci, fn_sci, tp_sci = cm_sci.ravel()\n",
    "sensitivity_sci = tp_sci / (tp_sci + fn_sci)\n",
    "specificity_sci = tn_sci / (tn_sci + fp_sci)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"SCIFOREST PERFORMANCE:\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Accuracy:    {accuracy_sci*100:.2f}%\")\n",
    "print(f\"Sensitivity: {sensitivity_sci*100:.2f}%\")\n",
    "print(f\"Specificity: {specificity_sci*100:.2f}%\")\n",
    "print(f\"\\nR result: Acc=84.68%, Sens=90.17%, Spec=30.98%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bb0639",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Density-Aware Isolation Forest\n",
    "\n",
    "**Key Idea:** Weight samples by local density - favor less dense regions\n",
    "\n",
    "**How it works:**\n",
    "1. Calculate density for each training sample (using k-nearest neighbors)\n",
    "2. Sample training data with bias toward low-density regions\n",
    "3. Train 100 models, each on differently sampled data\n",
    "4. Average anomaly scores\n",
    "\n",
    "**Expected from R:** Lower performance (specificity issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ffa6bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DENSITY-AWARE ISOLATION FOREST\n",
      "======================================================================\n",
      "\n",
      "Calculating local density for training samples...\n",
      "✓ Density weights calculated for 44,394 samples\n",
      "\n",
      "Training 100 models with density-based sampling...\n",
      "  Training tree 20/100...\n",
      "  Training tree 40/100...\n",
      "  Training tree 60/100...\n",
      "  Training tree 80/100...\n",
      "  Training tree 100/100...\n",
      "\n",
      "✓ Training complete in 1.30 seconds\n",
      "\n",
      "======================================================================\n",
      "DENSITY-AWARE IF PERFORMANCE:\n",
      "======================================================================\n",
      "Accuracy:    87.20%\n",
      "Sensitivity: 44.64%\n",
      "Specificity: 92.27%\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"DENSITY-AWARE ISOLATION FOREST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Calculate density weights\n",
    "print(\"\\nCalculating local density for training samples...\")\n",
    "k = 10\n",
    "nn = NearestNeighbors(n_neighbors=k)\n",
    "nn.fit(X_train_scaled)\n",
    "distances, _ = nn.kneighbors(X_train_scaled)\n",
    "knn_distances = distances[:, -1]  # Distance to k-th neighbor\n",
    "\n",
    "# Convert to density weights (inverse relation)\n",
    "density_weights = 1 / (knn_distances + 1e-6)\n",
    "density_weights = density_weights / density_weights.sum()\n",
    "\n",
    "# Invert for sampling (favor low density)\n",
    "sampling_weights = 1 / (density_weights + 1e-6)\n",
    "sampling_weights = sampling_weights / sampling_weights.sum()\n",
    "\n",
    "print(f\"✓ Density weights calculated for {len(density_weights):,} samples\")\n",
    "\n",
    "# Train ensemble\n",
    "n_trees = 100\n",
    "density_scores = np.zeros(X_test_scaled.shape[0])\n",
    "\n",
    "print(f\"\\nTraining {n_trees} models with density-based sampling...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(n_trees):\n",
    "    if (i + 1) % 20 == 0:\n",
    "        print(f\"  Training tree {i + 1}/{n_trees}...\")\n",
    "    \n",
    "    # Sample with density bias\n",
    "    sample_indices = np.random.choice(\n",
    "        len(X_train_scaled), \n",
    "        size=1000, \n",
    "        replace=True,\n",
    "        p=sampling_weights\n",
    "    )\n",
    "    sample_data = X_train_scaled[sample_indices]\n",
    "    \n",
    "    # Train model\n",
    "    density_model = IsolationForest(\n",
    "        n_estimators=1,\n",
    "        max_samples=1000,\n",
    "        contamination=contamination_rate,\n",
    "        random_state= random_seed + i\n",
    "    )\n",
    "    density_model.fit(sample_data)\n",
    "    \n",
    "    # Predict\n",
    "    predictions = density_model.predict(X_test_scaled)\n",
    "    density_scores += np.where(predictions == -1, 1, 0)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "density_scores = density_scores / n_trees\n",
    "\n",
    "# Apply threshold\n",
    "threshold = np.quantile(density_scores, 1 - contamination_rate)\n",
    "y_pred_density = np.where(density_scores >= threshold, 1, 0)\n",
    "\n",
    "print(f\"\\n✓ Training complete in {training_time:.2f} seconds\")\n",
    "\n",
    "# Evaluate\n",
    "accuracy_dens = accuracy_score(y_test, y_pred_density)\n",
    "cm_dens = confusion_matrix(y_test, y_pred_density)\n",
    "tn_dens, fp_dens, fn_dens, tp_dens = cm_dens.ravel()\n",
    "sensitivity_dens = tp_dens / (tp_dens + fn_dens)\n",
    "specificity_dens = tn_dens / (tn_dens + fp_dens)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"DENSITY-AWARE IF PERFORMANCE:\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Accuracy:    {accuracy_dens*100:.2f}%\")\n",
    "print(f\"Sensitivity: {sensitivity_dens*100:.2f}%\")\n",
    "print(f\"Specificity: {specificity_dens*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f162647",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Boxed Isolation Forest\n",
    "\n",
    "**Key Idea:** Create hierarchical feature boxes instead of using all features\n",
    "\n",
    "**How it works:**\n",
    "1. Train 100 models\n",
    "2. Each model uses a random \"box\" of 10-30 consecutive features\n",
    "3. Average all predictions\n",
    "\n",
    "**Why:** Groups related features together (temporal locality in R-R intervals)\n",
    "\n",
    "**Expected from R:** Poor specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be16b4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BOXED ISOLATION FOREST\n",
      "======================================================================\n",
      "\n",
      "Training 100 models with random feature boxes...\n",
      "  Training box 20/100...\n",
      "  Training box 40/100...\n",
      "  Training box 60/100...\n",
      "  Training box 80/100...\n",
      "  Training box 100/100...\n",
      "\n",
      "✓ Training complete in 1.71 seconds\n",
      "\n",
      "======================================================================\n",
      "BOXED IF PERFORMANCE:\n",
      "======================================================================\n",
      "Accuracy:    85.50%\n",
      "Sensitivity: 32.83%\n",
      "Specificity: 91.79%\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"BOXED ISOLATION FOREST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "n_boxes = 100\n",
    "box_scores = np.zeros(X_test_scaled.shape[0])\n",
    "\n",
    "print(f\"\\nTraining {n_boxes} models with random feature boxes...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(n_boxes):\n",
    "    if (i + 1) % 20 == 0:\n",
    "        print(f\"  Training box {i + 1}/{n_boxes}...\")\n",
    "    \n",
    "    # Create random box of features\n",
    "    box_size = np.random.randint(10, 31)  # Random size 10-30\n",
    "    max_start = 150 - box_size\n",
    "    start_feature = np.random.randint(0, max_start + 1)\n",
    "    box_features = list(range(start_feature, start_feature + box_size))\n",
    "    \n",
    "    # Train model on this feature box\n",
    "    box_model = IsolationForest(\n",
    "        n_estimators=1,\n",
    "        max_samples=1000,\n",
    "        contamination=contamination_rate,\n",
    "        random_state=random_seed + i\n",
    "    )\n",
    "    box_model.fit(X_train_scaled[:, box_features])\n",
    "    \n",
    "    # Predict\n",
    "    predictions = box_model.predict(X_test_scaled[:, box_features])\n",
    "    box_scores += np.where(predictions == -1, 1, 0)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "box_scores = box_scores / n_boxes\n",
    "\n",
    "# Apply threshold\n",
    "threshold = np.quantile(box_scores, 1 - contamination_rate)\n",
    "y_pred_boxed = np.where(box_scores >= threshold, 1, 0)\n",
    "\n",
    "print(f\"\\n✓ Training complete in {training_time:.2f} seconds\")\n",
    "\n",
    "# Evaluate\n",
    "accuracy_box = accuracy_score(y_test, y_pred_boxed)\n",
    "cm_box = confusion_matrix(y_test, y_pred_boxed)\n",
    "tn_box, fp_box, fn_box, tp_box = cm_box.ravel()\n",
    "sensitivity_box = tp_box / (tp_box + fn_box)\n",
    "specificity_box = tn_box / (tn_box + fp_box)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"BOXED IF PERFORMANCE:\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Accuracy:    {accuracy_box*100:.2f}%\")\n",
    "print(f\"Sensitivity: {sensitivity_box*100:.2f}%\")\n",
    "print(f\"Specificity: {specificity_box*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fef99a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Fair-Cut Forest\n",
    "\n",
    "**Key Idea:** Weight features inversely by variance to ensure fair representation\n",
    "\n",
    "**How it works:**\n",
    "1. Calculate variance for each feature\n",
    "2. Apply inverse variance weights (low variance features get higher weight)\n",
    "3. Train 100 models on weighted features\n",
    "4. Average predictions\n",
    "\n",
    "**Expected from R:** Poor performance (this had the variance bug!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "170ec2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FAIR-CUT FOREST\n",
      "======================================================================\n",
      "\n",
      "Calculated fair weights for 150 features\n",
      "Weight range: 0.006667 to 0.006667\n",
      "\n",
      "Training 100 models with fair-weighted features...\n",
      "  Training tree 20/100...\n",
      "  Training tree 40/100...\n",
      "  Training tree 60/100...\n",
      "  Training tree 80/100...\n",
      "  Training tree 100/100...\n",
      "\n",
      "✓ Training complete in 4.76 seconds\n",
      "\n",
      "======================================================================\n",
      "FAIR-CUT FOREST PERFORMANCE:\n",
      "======================================================================\n",
      "Accuracy:    86.75%\n",
      "Sensitivity: 38.01%\n",
      "Specificity: 92.57%\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"FAIR-CUT FOREST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate fair weights (inverse variance)\n",
    "feature_vars = X_train_scaled.var(axis=0)\n",
    "fair_weights = 1 / (feature_vars + 1e-6)\n",
    "fair_weights = fair_weights / fair_weights.sum()\n",
    "\n",
    "print(f\"\\nCalculated fair weights for {len(fair_weights)} features\")\n",
    "print(f\"Weight range: {fair_weights.min():.6f} to {fair_weights.max():.6f}\")\n",
    "\n",
    "n_trees = 100\n",
    "faircut_scores = np.zeros(X_test_scaled.shape[0])\n",
    "\n",
    "print(f\"\\nTraining {n_trees} models with fair-weighted features...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(n_trees):\n",
    "    if (i + 1) % 20 == 0:\n",
    "        print(f\"  Training tree {i + 1}/{n_trees}...\")\n",
    "    \n",
    "    # Apply fair weights to features\n",
    "    weighted_train = X_train_scaled * fair_weights\n",
    "    weighted_test = X_test_scaled * fair_weights\n",
    "    \n",
    "    # Train model\n",
    "    faircut_model = IsolationForest(\n",
    "        n_estimators=1,\n",
    "        max_samples=1000,\n",
    "        contamination=contamination_rate,\n",
    "        random_state=random_seed + i\n",
    "    )\n",
    "    faircut_model.fit(weighted_train)\n",
    "    \n",
    "    # Predict\n",
    "    predictions = faircut_model.predict(weighted_test)\n",
    "    faircut_scores += np.where(predictions == -1, 1, 0)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "faircut_scores = faircut_scores / n_trees\n",
    "\n",
    "# Apply threshold\n",
    "threshold = np.quantile(faircut_scores, 1 - contamination_rate)\n",
    "y_pred_faircut = np.where(faircut_scores >= threshold, 1, 0)\n",
    "\n",
    "print(f\"\\n✓ Training complete in {training_time:.2f} seconds\")\n",
    "\n",
    "# Evaluate\n",
    "accuracy_fc = accuracy_score(y_test, y_pred_faircut)\n",
    "cm_fc = confusion_matrix(y_test, y_pred_faircut)\n",
    "tn_fc, fp_fc, fn_fc, tp_fc = cm_fc.ravel()\n",
    "sensitivity_fc = tp_fc / (tp_fc + fn_fc)\n",
    "specificity_fc = tn_fc / (tn_fc + fp_fc)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"FAIR-CUT FOREST PERFORMANCE:\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Accuracy:    {accuracy_fc*100:.2f}%\")\n",
    "print(f\"Sensitivity: {sensitivity_fc*100:.2f}%\")\n",
    "print(f\"Specificity: {specificity_fc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4da7fa0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FINAL RESULTS - ALL MODELS\n",
      "======================================================================\n",
      "\n",
      "PYTHON RESULTS:\n",
      "======================================================================\n",
      "         Algorithm  Accuracy (%)  Sensitivity (%)  Specificity (%)\n",
      "     Random Forest         98.14            84.62            99.75\n",
      "       Standard IF         86.54            36.57            92.51\n",
      "Single-Variable IF         84.63            27.98            91.41\n",
      "         SCiForest         86.25            36.04            92.24\n",
      "  Density-Aware IF         87.20            44.64            92.27\n",
      "          Boxed IF         85.50            32.83            91.79\n",
      "   Fair-Cut Forest         86.75            38.01            92.57\n",
      "\n",
      "\n",
      "R RESULTS (from your paper):\n",
      "======================================================================\n",
      "\n",
      "Algorithm                 Accuracy (%)  Sensitivity (%)  Specificity (%)\n",
      "Random Forest                   84.22            87.91            54.83\n",
      "Standard IF                     85.45            92.36            30.47\n",
      "Single-Variable IF              85.70            92.52            31.45\n",
      "SCiForest                       84.68            90.17            30.98\n",
      "\n",
      "\n",
      "======================================================================\n",
      "KEY FINDINGS:\n",
      "======================================================================\n",
      "\n",
      "1. RANDOM FOREST:\n",
      "   Python: Acc=98.14%, Sens=84.62%, Spec=99.75%\n",
      "   R:      Acc=84.22%, Sens=87.91%, Spec=54.83%\n",
      "   → Python MUCH better! (+14% accuracy, +45% specificity)\n",
      "\n",
      "2. ISOLATION FOREST VARIANTS:\n",
      "   Python: High specificity (91-92%), LOW sensitivity (28-45%)\n",
      "   R:      Low specificity (30-31%), HIGH sensitivity (90-92%)\n",
      "   → Completely OPPOSITE performance profiles!\n",
      "\n",
      "3. WINNER:\n",
      "   Python: Random Forest dominates ALL metrics\n",
      "   R:      Isolation Forest variants had best sensitivity\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"FINAL RESULTS - ALL MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create results dataframe\n",
    "results = {\n",
    "    'Algorithm': [\n",
    "        'Random Forest',\n",
    "        'Standard IF',\n",
    "        'Single-Variable IF',\n",
    "        'SCiForest',\n",
    "        'Density-Aware IF',\n",
    "        'Boxed IF',\n",
    "        'Fair-Cut Forest'\n",
    "    ],\n",
    "    'Accuracy (%)': [\n",
    "        98.14,\n",
    "        86.54,\n",
    "        84.63,\n",
    "        86.25,\n",
    "        87.20,\n",
    "        85.50,\n",
    "        86.75\n",
    "    ],\n",
    "    'Sensitivity (%)': [\n",
    "        84.62,\n",
    "        36.57,\n",
    "        27.98,\n",
    "        36.04,\n",
    "        44.64,\n",
    "        32.83,\n",
    "        38.01\n",
    "    ],\n",
    "    'Specificity (%)': [\n",
    "        99.75,\n",
    "        92.51,\n",
    "        91.41,\n",
    "        92.24,\n",
    "        92.27,\n",
    "        91.79,\n",
    "        92.57\n",
    "    ]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\nPYTHON RESULTS:\")\n",
    "print(\"=\"*70)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nR RESULTS (from your paper):\")\n",
    "print(\"=\"*70)\n",
    "r_results = \"\"\"\n",
    "Algorithm                 Accuracy (%)  Sensitivity (%)  Specificity (%)\n",
    "Random Forest                   84.22            87.91            54.83\n",
    "Standard IF                     85.45            92.36            30.47\n",
    "Single-Variable IF              85.70            92.52            31.45\n",
    "SCiForest                       84.68            90.17            30.98\n",
    "\"\"\"\n",
    "print(r_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY FINDINGS:\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n1. RANDOM FOREST:\")\n",
    "print(f\"   Python: Acc=98.14%, Sens=84.62%, Spec=99.75%\")\n",
    "print(f\"   R:      Acc=84.22%, Sens=87.91%, Spec=54.83%\")\n",
    "print(f\"   → Python MUCH better! (+14% accuracy, +45% specificity)\")\n",
    "\n",
    "print(\"\\n2. ISOLATION FOREST VARIANTS:\")\n",
    "print(f\"   Python: High specificity (91-92%), LOW sensitivity (28-45%)\")\n",
    "print(f\"   R:      Low specificity (30-31%), HIGH sensitivity (90-92%)\")\n",
    "print(f\"   → Completely OPPOSITE performance profiles!\")\n",
    "\n",
    "print(\"\\n3. WINNER:\")\n",
    "print(f\"   Python: Random Forest dominates ALL metrics\")\n",
    "print(f\"   R:      Isolation Forest variants had best sensitivity\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
